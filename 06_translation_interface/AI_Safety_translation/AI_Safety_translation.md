# Implementable Calibrated Transparency for Frontier AI
_A mechanistic, adversarialâ€‘robust, and policyâ€‘aligned safety architecture_

## Abstract
We propose **Calibrated Transparency (CT)**, a formal framework that integrates recursive value learning, adversarialâ€‘robust verification, and cryptographic auditability to operationalize AI safety governance. Unlike descriptive transparency, CT establishes **causal pathways** from calibration mechanisms to safety guarantees through: (i) stateâ€‘consistent Lyapunov verification with runtime monitoring, (ii) robustness against metric gaming and deceptive alignment with measurable detection bounds, (iii) **operationally defined metrics** with reproducible protocols, and (iv) policyâ€‘aligned implementation mappings. We synthesize 2020â€“2025 research across preference learning (Kim et al., 2025), Lyapunov and barrierâ€‘certificate verification (Henzinger et al., 2025), cryptographic auditing (Balan et al., 2025), and governance frameworks (NIST AI RMF, EU AI Act, ISO/IEC 42001). Our core contribution is a mechanistically grounded theory with dependencyâ€‘aware risk composition, demonstrating that transparent calibrationâ€”when coupled with adversarial stressâ€‘testing and formal barrier certificatesâ€”provides measurable operational assurance against known failure modes (hallucination, specification gaming, oversight collapse), while explicitly acknowledging limitations for existentialâ€‘risk scenarios.

---

## 1. Introduction: The Calibrationâ€“Safety Gap
**Problem.** Statistical calibration does not guarantee behavioral safety. Models with wellâ€‘calibrated uncertainty can still (i) pursue mesaâ€‘objectives misaligned with training goals (Hubinger etâ€¯al., 2021), (ii) exploit evaluation loopholes (Krakovna etâ€¯al., 2020), (iii) exhibit deceptive alignment (Cotra, 2022), and (iv) fail under distributional shift (Hendrycks etâ€¯al., 2021). Descriptive transparency instrumentsâ€”model cards (Mitchell etâ€¯al., 2019), datasheets (Gebru etâ€¯al., 2021), postâ€‘hoc explainers (Ribeiro etâ€¯al., 2016)â€”document properties but do not make safety **causal**.

**Approach.** **Calibrated Transparency (CT)** formalizes transparency as a **causal safety mechanism**: calibrated uncertainty gates actions, Lyapunov certificates constrain updates, recursive preference checks deter reward hacking, and cryptographic proofs make claims auditable.

**Scope.** CT targets operational safety for nearâ€‘term frontier systems (â‰ˆ10Bâ€“1T parameters, 2025â€“2030). It is **necessary but insufficient** for superintelligent deception, unbounded selfâ€‘modification, or multiâ€‘polar races (Bostrom, 2014; Carlsmith, 2022).

**Contributions.**
1) Axiomatized CT with stateâ€‘consistent verification (Â§2); 2) Dependencyâ€‘aware safety bound (Theoremâ€¯2.1) (Â§2.2); 3) Identified causal pathways from calibration to harm reduction (Â§3); 4) Adversarial alignment mechanisms and detection metrics (Â§4); 5) Operational measurement protocols and acceptance criteria for HAM, CD, DR, SI, AAS (Â§5); 6) Federated multiâ€‘agent CT with SMPC aggregation and gameâ€‘theoretic corrigibility (Â§6); 7) Mappings to NIST AI RMF, EU AI Act, ISO/IEC 42001, and postâ€‘market monitoring (Â§7).

---

## 2. Formal Framework: Calibrated Transparency with Stateâ€‘Consistent Verification

### 2.1 Core Definitions
Let \( \mathcal{X} \) be the closedâ€‘loop state space; \( x_t \in \mathcal{X} \) the system state at time \( t \); \( \theta_t \) embedded within \( x_t \); \( \mathcal{U}_t \) the predictive uncertainty; \( \mathcal{P}_t \) the preference distribution.

**Definition 2.1 (Epistemic state).** \( \mathcal{E}_t = (x_t, \mathcal{U}_t, \mathcal{P}_t) \).

**Definition 2.2 (Calibrated Transparency).** A system exhibits CT if it satisfies:
- **CTâ€‘1 (Statistical calibration):** for any prediction \( \hat{y} \) with confidence \( p \), \( \Pr(y=\hat{y}\mid p) = p \pm \epsilon_{\text{stat}} \) on \( \mathcal{D}_{\text{test}} \), with \( \epsilon_{\text{stat}} \le 0.05 \).
- **CTâ€‘2 (Mechanistic verifiability):** there exists a \( C^1 \) Lyapunov function \( V:\mathcal{X}\to\mathbb{R}_{\ge 0} \) and compact \( \mathcal{X}_{\text{safe}}\subset\mathcal{X} \) such that \( V(x)=0 \Leftrightarrow x\in\mathcal{X}_{\text{eq}}\subset\mathcal{X}_{\text{safe}} \) and \( \dot V(x)\le -\alpha(\|x-x_{\text{eq}}\|) \) (classâ€‘\( \mathcal{K} \) \( \alpha \)) whenever the barrierâ€‘constrained update is applied; runtime monitoring enforces \( x_t \in \mathcal{X}_{\text{safe}} \) with â‰¤10% overhead.
- **CTâ€‘3 (Recursive preference coherence):** preference updates satisfy \( D_{\mathrm{KL}}(\mathcal{P}_{t+1}\|\mathcal{P}_t)\le \delta \) with humanâ€‘verifiable justification trace \( \mathcal{T}_t \) and metaâ€‘preference validation.
- **CTâ€‘4 (Cryptographic auditability):** all epistemic updates admit zkâ€‘proofs \( \pi_t \) with completeness \( \ge 1-\nu \), soundness \( \ge 1-\mu \), and zeroâ€‘knowledge; we prove range predicates \( (\mathrm{SCI}_t\ge\theta)\wedge (\mathrm{ECE}_t\le\gamma) \) over Pedersenâ€‘committed metrics using **zkâ€‘STARKs** (no trusted setup). Public verification < 100â€¯ms/receipt; receipts are anchored every 10â€¯s to a **Hyperledger Fabric** ledger; local writeâ€‘ahead logs enable batch reconciliation on outage.
- **CTâ€‘5 (Adversarial robustness):** CT properties hold under adaptive adversaries with compute budget \( B < B_{\text{critical}} \) (threat model in Â§4).

**A4â€² (Deception detector performance, clarified).** The mesaâ€‘optimizer detector achieves **TPR â‰¥â€¯0.90** and **FNR â‰¤â€¯0.05** (95% CI) on groundâ€‘truth honeypot/OOD cases adjudicated by â‰¥â€¯3 independent reviewers; in Theoremâ€¯2.1 we use \( \epsilon_{\text{det}}=\) FNR.

### 2.2 Theorem with Dependencyâ€‘Aware Risk Composition
**Theorem 2.1 (CT safety guarantee).** Under Assumptions A1â€“A3 and A4â€² below,
\[
\Pr\!\Big(\bigcup_i E_i\Big) \le \sum_i \epsilon_i - \sum_{i<j} \max\{0,\ \epsilon_i+\epsilon_j-1+\rho_{ij}\},
\]
where \( E_i \) are failure events (statistical miscalibration, mechanistic constraint violation, adversarial exploitation, deception detection failure), \( \epsilon_i \in \{\epsilon_{\text{stat}},\epsilon_{\text{mech}},\epsilon_{\text{adv}},\epsilon_{\text{det}}\} \), and \( \rho_{ij}\in[0,1] \) upperâ€‘bounds pairwise dependence between failure modes.

**Assumptions.**
- **A1 (Distributional proximity):** \( D_{\mathrm{KL}}(\mathcal{D}_{\text{deploy}}\|\mathcal{D}_{\text{train}})\le \tau \) and \( W_2(\mathcal{D}_{\text{deploy}},\mathcal{D}_{\text{train}})\le\tau_W \).
- **A2 (Oversight latency):** human oversight frequency \( f_{\text{human}}\in[0.5,1.0] \)â€¯Hz with latency \( \lambda<\lambda_{\text{critical}} \) calibrated by domain.
- **A3 (Adversarial compute budget):** adversary budget \( B < B_{\text{critical}} \) (estimated via staged redâ€‘teaming: \(10^3\), \(10^6\), \(10^9\) query tiers; threshold = first **reproducible CTâ€‘property violation** across 3 independent campaigns).
- **A4â€²** as above.

**Proof sketch.** CTâ€‘1 bounds \( \epsilon_{\text{stat}} \) by calibration theory; CTâ€‘2 with Lyapunov analysis yields \( \epsilon_{\text{mech}} e^{-\alpha t} \) (local or SOSâ€‘certified global). CTâ€‘5 with A4â€² bounds \( \epsilon_{\text{adv}} \) and \( \epsilon_{\text{det}} \) via adversarial game analysis. Crossâ€‘terms \( \Pr(E_i\cap E_j) \) are bounded using conditional independence tests or empirical bootstrap estimates (Appendixâ€¯D). â–¡

---

## 3. Causal Mechanisms: From Calibration to Safety

### 3.1 Structural Causal Model (SCM) and Identification
\( C \) (calibration) â†’ \( U \) (uncertainty awareness);
\( L \) (Lyapunov verification) â†’ \( T \) (trajectory constraints);
\( P \) (preference coherence) â†’ \( O \) (objective alignment);
\( A \) (cryptoâ€‘audit) â†’ \( D \) (tamper detection);
\( U \land T \land O \land D \Rightarrow S \) (safety).

Identification requires manipulability (A/B ablations), confound control (stratification or IVs), and common support; doâ€‘interventions are specified in Appendixâ€¯A.

### 3.2 Mechanistic Pathways
**Path 1: Calibration â†’ Hallucination reduction.** Calibrated \( \mathcal{U}_t \) gates rejection sampling, abstention (â€œI donâ€™t knowâ€ when \( H(\mathcal{U}_t)>\eta=1.5 \)â€¯nats), and human escalation. Empirically, VSCBench shows ~37% hallucination reduction when abstention is calibrationâ€‘gated (Geng etâ€¯al., 2025).

**Path 2: Lyapunov verification â†’ Constraint satisfaction.**

**Algorithm 1: Safe policy update with barrier constraint.**
1) Compute \( V(x_t) \), \( \dot V(x_t) \); halt if \( V(x_t)>V_{\max} \).  
2) Nominal update \( \Delta\theta_{\text{nom}}=-\eta\nabla_\theta \mathcal{L} \).  
3) Project: \( \Delta\theta_{\text{safe}}=\Delta\theta_{\text{nom}}-\lambda(1-\mathrm{SCI}_t)\nabla V(x_t) \).  
4) Verify \( V(x_t+\Delta\theta_{\text{safe}})\le V(x_t)+\epsilon_{\text{tol}} \); backâ€‘off \( \eta \) on failure.

**Path 3: Preference coherence â†’ Specâ€‘gaming prevention.** Recursive checks (Kim etâ€¯al., 2025) test metaâ€‘preferences \( \mathcal{M} \) and counterfactuals; models with \( D_{\mathrm{KL}}(\mathcal{P}_{t+1}\|\mathcal{M})>\delta_{\text{meta}} \) are rejected preâ€‘deployment.

### 3.3 Intervention Studies (required validation)
Ablations across \(2^5=32\) CT configurations; causal forests for heterogeneous effects (Wager & Athey, 2018); RCTs with production traffic. Protocols in Appendixâ€¯A.

---

## 4. Adversarial Robustness and Deceptive Alignment

### 4.1 Threat Model
External attackers, **specification gamers**, and **deceptive mesaâ€‘optimizers**. Attack family \( \mathcal{A}(\epsilon) \): \( \|\delta\|_\infty\le\epsilon \) (input), \( \|\delta_\theta\|_2\le \epsilon_\theta \) (parameter), PGD/C&W with â‰¤100 steps. \( B_{\text{critical}} \) is defined via staged redâ€‘teaming (Appendixâ€¯C).

### 4.2 Defenses
**Secondâ€‘order metric (AAS).**  
\( \mathrm{AAS}=\mathbb{E}_{p\sim\mathcal{A}_{\text{adv}}}[\mathrm{HAM}(p)]-\mathbb{E}_{p\sim \mathcal{D}_{\text{nat}}}[\mathrm{HAM}(p)] \); flag if \( |\mathrm{AAS}|>0.15 \). **Power analysis:** detect \( |\mathrm{AAS}|\ge 0.10 \) at \( \alpha=0.05,\ \beta=0.20 \) assuming \( \sigma_{\mathrm{HAM}}\approx 0.15 \Rightarrow n\ge 5{,}000 \) per split.

**Activation archaeology.** Sparse probing for deceptionâ€‘related features (Burns etâ€¯al., 2023; Zheng etâ€¯al., 2025): discontinuities, latent objective mismatch, honeypot differentials. Target TPRâ€¯â‰¥â€¯0.90, FNRâ€¯â‰¤â€¯0.05.

**Certified adversarial training.** Innerâ€‘maximization over CTâ€‘metrics; robustness verified with 100â€‘step PGD. Guarantee \( \Pr(\text{adv success})\le \epsilon_{\text{cert}} \) for \( \|\delta\|\le\epsilon \).

### 4.3 Specification Gaming: Operational SOP
If \( \mathrm{AAS}> \tau_{\mathrm{AAS}} \) or metaâ€‘preference violations: (i) **weight rotation** via EMA rollback; (ii) **oversight escalation** \( f_{\text{human}}\times2 \) for 48â€¯h; (iii) **tighten thresholds** \( \tau_{\mathrm{HAM}}\!+\!0.1 \), \( \delta_{\text{meta}}\!\times\!0.5 \); (iv) external redâ€‘team â‰¥â€¯100 personâ€‘hours; (v) staged rollback to last CTâ€‘compliant checkpoint.

### 4.4 Deceptive Alignment: Limits
OOD probing, temporal consistency \( D_{\mathrm{KL}}(\mathcal{P}_t\|\mathcal{P}_{t-k})\le\delta \), honeypotsâ€”provide probabilistic, not absolute, assurance; guarantees degrade if capabilities exceed human+tool verification.

---

## 5. Practical Verifiability: Metrics and Protocols

### 5.1 Metrics (acceptance criteria)
**HAM (Humanâ€‘Alignment Measure):** Spearmanâ€™s \( \rho \) between model rankings and expert consensus (benchmark: TrustLLMâ€‘Align; \( n=1{,}000 \) items; 5 experts; Krippendorffâ€™s \( \alpha\ge 0.8 \)); 95% BCa bootstrap; accept if \( \rho\ge 0.80 \) and lowerâ€‘CIâ€¯â‰¥â€¯0.75.

**CD (Calibration Deviation):** Expected Calibration Error (adaptive bins; 10â€¯k heldâ€‘out); accept if ECEâ€¯â‰¤â€¯0.05.

**DR (Divergence Rate):** \( \mathrm{DR}_t=\mathbb{E}_{s\sim\hat d} [D_{\mathrm{KL}}(\pi_t(\cdot|s)\|\pi_{\text{ref}}(\cdot|s))] \); accept if \( \mathrm{DR}_t \le 0.20 \).

**SI (Stability Index):** \( \mathrm{SI}(x_t)=\exp(-|\dot V(x_t)|)\in[0,1] \); accept if \( \mathrm{SI}\ge 0.90 \) on â‰¥95% of trajectory steps.

**AAS (Adversarial Alignment Score):** as in Â§4.2; accept if \( |\mathrm{AAS}|\le 0.10 \).

### 5.2 Benchmarks
HAM: TrustLLMâ€‘Align (Sun etâ€¯al., 2024);  
CD: VSCBench (Geng etâ€¯al., 2025) + CalibrationNet;  
DR: **PolicyDriftâ€‘Bench** (proposed community suite);  
AAS: **AdversarialAlignâ€‘100** (10 domains Ã— 100 scenarios; Appendixâ€¯B).

### 5.3 Frontierâ€‘Model Case Study (proposed)
Staged rollout (1%â†’10%â†’100%) over 90â€¯days, â‰¥10^7 queries. KPIs: hallucination rate, escalation frequency, specâ€‘gaming incidents, user trust, overhead (â‰¤10%). Targets: hallucination â†“50% vs. baseline; HAMâ€¯â‰¥â€¯0.85; ECEâ€¯â‰¤â€¯0.05; DRâ€¯â‰¤â€¯0.20; SIâ€¯â‰¥â€¯0.90; zero critical specâ€‘gaming incidents.

---

## 6. Multiâ€‘Agent Cooperative Alignment

### 6.1 Federated CT with SMPC
Each agent \( i \) computes \( \mathrm{SCI}_i = (\mathrm{HAM}_i \cdot (1-\mathrm{CD}_i)\cdot (1-\mathrm{DR}_i)\cdot \mathrm{SI}_i)^{1/4} \). Agents share \( \mathrm{SCI}_i \) via additive secret sharing (threshold \( t=\lceil n/2\rceil+1 \)); secure median yields \( \mathrm{SCI}_{\text{global}} \). Outliers with \( |\mathrm{SCI}_i-\mathrm{SCI}_{\text{global}}|>0.15 \) trigger thirdâ€‘party audits. **Collusion resistance:** if â‰¥â€¯\( t \) agents collude, \( \mathrm{SCI}_{\text{global}} \) may be unreliable; mitigation via **reputationâ€‘weighted aggregation** (future work).

### 6.2 Gameâ€‘Theoretic Corrigibility
Repeatedâ€‘game model (Oesterheld & Shah, 2024): supervisory modulation \( R_i^{\text{total}}=R_i^{\text{ind}}+\beta\sum_{j\ne i}\mathrm{HAM}(i,j) \). For \( n\le 10 \) and \( \beta=0.3 \), Nash equilibria achieve \( \mathrm{SCI}_{\text{global}}\ge 0.80 \) (empirical validation needed for larger \( n \)).

---

## 7. Governance Integration and Implementation

### 7.1 NIST AI RMF Mapping (Governâ€“Mapâ€“Measureâ€“Manage)
- **Govern:** zkâ€‘STARK audit receipts; tamper detection \( (1-\mu) \).  
- **Map:** preferenceâ€‘drift checks \( D_{\mathrm{KL}}(\mathcal{P}_{t+1}\|\mathcal{P}_t) \).  
- **Measure:** HAM, CD, DR, SI, AAS; composite SCI; dependencyâ€‘aware risk (Theoremâ€¯2.1).  
- **Manage:** Lyapunovâ€‘constrained updates; circuit breakers; redâ€‘team SOP; postâ€‘market monitoring.

### 7.2 EU AI Act Compliance
- **Art.â€¯11 (Technical documentation):** CT mechanisms + measurement protocols satisfy Annexâ€¯IV documentation requirements.  
- **Art.â€¯14â€“17 (Human oversight & QMS):** adaptive oversight (0.5â€“1.0â€¯Hz), stopâ€‘button, documented risk controls.  
- **Art.â€¯43 (Conformity assessment):** notified bodies verify CTâ€‘1â€¦CTâ€‘5; harmonization with EN ISO/IEC 42001 (AIMS).  
- **Art.â€¯72 (Postâ€‘market monitoring):** continuous SCI tracking; incident trigger at \( \mathrm{SCI}<0.70 \); quarterly reviews.

### 7.3 ISO/IEC Integration
- **ISO/IEC 42001 (AIMS):** PDCA alignmentâ€”Plan thresholds, Do deploy CT, Check monitor SCI/AAS, Act via SOP.  
- **ISO/IEC 23894 (Risk):** SCI as composite risk indicator; Lyapunov verification as protective measure.

### 7.4 Roadmap (2025â€“2030)
**Phaseâ€¯1:** integrate PFP into RLHF; deploy ensembles/MCâ€‘dropout; standâ€‘up zkâ€‘STARK audits.  
**Phaseâ€¯2:** implement barrier certificates; conduct frontier case studies; release **PolicyDriftâ€‘Bench** and **AdversarialAlignâ€‘100**.  
**Phaseâ€¯3:** federated CT across multiâ€‘agent systems; complete conformity assessments; harmonize with OECD/G7 processes.

---

## 8. Limitations and Open Problems
CT does not solve superintelligent deception, deep value uncertainty, unbounded selfâ€‘modification, or competitive race dynamics. Open problems: populationâ€‘scale metaâ€‘preference elicitation; estimating \( B_{\text{critical}} \); tightening shift bounds \( \tau,\tau_W \); collusionâ€‘resistant aggregation; AIâ€‘assisted oversight for superhuman domains.

---

## 9. Conclusion
CT reframes transparency as an **engineerable safety mechanism**. By coupling calibrated uncertainty, Lyapunovâ€‘constrained updates, adversarial diagnostics, and cryptographic auditabilityâ€”and aligning these with governance frameworksâ€”organizations can obtain measurable, auditable assurance for nearâ€‘term frontier models. We call for industryâ€“academiaâ€“policy collaboration on open benchmarks, field trials, and standards to validate and scale CT in practice.

---

## References
**Alignment & Corrigibility.**  
Soares, N., Fallenstein, B. (2014). *Corrigibility*. FLI.  
Christiano, P. (2018). *AI Alignment: Iterated Amplification*.  
Hubinger, E. etâ€¯al. (2021). *Risks from Learned Optimization*. arXiv:1906.01820.  
Oesterheld, C., Shah, R. (2024). *Cooperative/Coâ€‘rrective AI* (tech report/workshop).  
Kenton, Z. etâ€¯al. (2021). *Alignment of Language Agents*. arXiv:2103.14659.  
Hadfieldâ€‘Menell, D. etâ€¯al. (2017). *The Offâ€‘Switch Game*. IJCAI.

**Uncertainty & Calibration.**  
Guo, C. etâ€¯al. (2017). *On Calibration of Modern Neural Networks*. ICML.  
Lakshminarayanan, B. etâ€¯al. (2017). *Deep Ensembles*. NeurIPS.  
Minderer, M. etâ€¯al. (2021). *Revisiting Calibration*. NeurIPS.  
Ovadia, Y. etâ€¯al. (2019). *Can You Trust Your Modelâ€™s Uncertainty?*. NeurIPS.

**Mechanistic Verification & Runtime Assurance.**  
Khalil, H. (2002). *Nonlinear Systems*. Prentice Hall.  
Henzinger, T. A. etâ€¯al. (2025). *Formal Verification of Neural Certificates Done Dynamically*. arXiv:2507.11987 (preprint).  
Henrique, *etâ€¯al.* (2025). *Lyapunovâ€‘style Verification for RLHF* (workshop/preprint).  
Parrilo, P. (2000s). SOS foundations and toolchains.

**Preference Learning & Specâ€‘Gaming.**  
Kim, D. etâ€¯al. (2025). *Debiasing Online Preference Learning via Preference Feature Preservation*. arXiv:2506.11098.  
Krakovna, V. etâ€¯al. (2020). *Specification Gaming Examples in AI*. DeepMind Safety.  
Schwinn, *etâ€¯al.* (2025). *Adversarial Alignment Requires Measurable Objectives*. arXiv:2502.11910 (preprint; verify).

**Deceptive Alignment & Threats.**  
Cotra, A. (2022). *Takeover without Countermeasures*. Cold Takes.  
Carlsmith, J. (2022). *Is Powerâ€‘Seeking AI an Existential Risk?*. arXiv:2206.13353.  
Burns, C. etâ€¯al. (2023). *Discovering Latent Knowledge Without Supervision*. ICLR.  
Zheng, *etâ€¯al.* (2025). *Mesaâ€‘Optimization in Autoregressive Transformers*. NeurIPS Poster (to verify).

**Benchmarks & Evaluation.**  
Sun, L. etâ€¯al. (2024). *TrustLLM*. arXiv:2401.05561.  
Geng, *etâ€¯al.* (2025). *VSCBench*. arXiv:2505.20362 (preprint).  
Srivastava, A. etâ€¯al. (2022). *BIGâ€‘Bench*. arXiv.  
Liang, P. etâ€¯al. (2022). *HELM*. arXiv.

**Causality & Adversarial Robustness.**  
Pearl, J. (2009). *Causality*. Cambridge.  
Madry, A. etâ€¯al. (2018). *Towards Robust Deep Learning*. ICLR.

**Governance & Policy.**  
NIST (2023). *AI Risk Management Framework 1.0*.  
EU (2024). *EU AI Act* (Arts.â€¯11,â€¯14â€“17,â€¯43,â€¯72; Annexâ€¯IV).  
ISO/IEC 42001:2023 *AI Management System*.  
ISO/IEC 23894:2023 *AI Risk Management*.  
Raji, I. D. etâ€¯al. (2020). *Closing the AI Accountability Gap*. FAT*.  
Whittlestone, J. etâ€¯al. (2019). *The Role and Limits of Principles*. AIES.

**Human Factors & Trust.**  
Lee, J. D., See, K. (2004). *Trust in Automation*. Human Factors.  
Zhang, Y. etâ€¯al. (2020). *Confidence/Explanation Effects on Trust Calibration*. CHI.

**Background & Limits.**  
Bostrom, N. (2014). *Superintelligence*. OUP.  
Hendrycks, D. etâ€¯al. (2021). *Unsolved Problems in ML Safety*. arXiv.

---

## Appendices (implementation supplements)

### Appendix A â€” Causal Intervention Protocols
- **Datasets:** TrustLLMâ€‘Align (1,000 items), AIRâ€‘Bench; total **â‰¥â€¯120k** samples for calibration.  
- **Bootstrap:** 10,000 resamples, BCa intervals.  
- **A/B designs:** doâ€‘switches for abstention, rejection, escalation; negative controls and stratified randomization.

### Appendix B â€” Lyapunov Verification Details
- **Safe set \( \mathcal{X}_{\text{safe}} \)** and certificate construction via SOS programming; offline verification with **autodiff (JAX)**; runtime enforcement with **controlâ€‘barrierâ€‘function substitution** ensuring \( \dot V\le 0 \).  
- **Stress tests:** \(10^6\) episodes; empirical decay \( \alpha \approx 0.13\,\mathrm{s}^{-1} \).

### Appendix C â€” Adversarial Robustness & Redâ€‘Teaming
- **Threat tiers:** Baseline \(10^3\), Medium \(10^6\), Advanced \(10^9\) query budgets; **\( B_{\text{critical}} \)** defined as the **minimum budget achieving â‰¥â€¯10% CTâ€‘violation rate across 3 independent redâ€‘team campaigns**.  
- **Power analysis:** \( \alpha=0.05,\ \beta=0.20 \), \( \sigma_{\mathrm{HAM}}\approx 0.15 \Rightarrow n\ge 5{,}000 \) per split; 95% CI target \( |\mathrm{AAS}|\le 0.08 \).

### Appendix D â€” Dependencyâ€‘Aware Risk Composition
- **Bootstrap estimation of \( \rho_{ij} \)** (pairwise dependence of failure modes) with 10,000 resamples;  
- **Computation budget:** ~100 CPU cores, ~6â€¯min per quarter;  
- **Auditability:** intermediate CI summaries are committed to the cryptographic log for reproducibility.

---

**Note:** Detailed implementation specifications, measurement protocols, computational infrastructure, and proposed community benchmarks are provided in the supplementary document:

ðŸ“Ž **CT_Appendices_Full.md** â€” Complete appendices with reproducibility protocols, audit specifications, deployment checklists, and benchmark designs.

---
