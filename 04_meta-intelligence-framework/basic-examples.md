# Meta-Intelligence Framework: Basic Application Examples

**Version:** alpha  
**Last Updated:** October 2025

## Overview
This document shows how the **Meta-Intelligence Integration Framework** works in real contexts.  
Each example now follows a unified structure:
1) Initial Diagnosis, 2) Layer-Specific Outputs, 3) Inter-Layer Interactions, 4) Learning Points, 5) Evaluation Metrics & Rubric, 6) Cultural Adaptation Deep Dive (where applicable)

---

## Quick Diagnosis
Before diving into examples, identify which layers you need:

- Is your problem well-defined? → ❌ **Start with Layer I**
- Do you understand stakeholders? → ❌ **Use Layer II**
- Do you have an actionable plan? → ❌ **Engage Layer III**
- Have you checked ethics? → ❌ **Layer IV mandatory**

---

## Contents
1. Example 1 — Designing an AI Ethics Curriculum  
2. Example 2 — Supporting Medical Decision-Making  
3. Example 3 — Hiring Strategy in a Startup  
4. Cross-Domain Patterns  
5. Try It Yourself  
6. Global Evaluation Rubric (All Examples)

---

## Example 1 | Designing an AI Ethics Curriculum

### 1) Initial Diagnosis
| Question | Assessment | Missing Layer |
|---|---|---|
| Clear theoretical framework? | ❌ Ambiguous scope | I (Structural) |
| Student backgrounds considered? | ❌ No humanities/STEM split | II (Contextual) |
| Feasible weekly plan? | ❌ 15-week schedule missing | III (Integrative) |
| Ethical bias audit? | ❌ Untested value orientations | IV (Reflective) |
→ All layers required; emphasize **II** and **IV**.

---

### 2) Layer-Specific Outputs

### Phase 1 — Structural Intelligence
#### Prompt Example
“Summarize standard topics in AI ethics referencing IEEE/ACM. Provide a coherent taxonomy.”
#### Sample Output
1. Fairness  2. Transparency  3. Privacy  4. Safety  5. Social Impact  
#### Layer Contribution
Provided the conceptual skeleton.

### Phase 2 — Contextual Intelligence
#### Prompt Example
“Given 50% humanities, 50% STEM, 30% international, and mixed career goals, localize the topics.”
#### Output Summary (~100 words)
Humanities: start with normative questions (justice, rights).  
STEM: introduce fairness metrics and hands-on bias detection.  
International students: contrast individualism/collectivism; include Asian/African cases.  
Industry-focused: connect to governance, compliance, and risk.  
Provide differentiated reading lists and assignments by track while keeping shared ethics labs for cross-pollination. Add bilingual glossaries for key terms and cultural notes for sensitive topics. *(Full details in supplementary materials.)*
#### Layer Contribution
Aligned content with learner contexts and goals.

### Phase 3 — Integrative Intelligence
#### Prompt Example
“Design a 15-week syllabus (5 theory, 5 case, 5 practice), with assessment and guest options.”
#### Sample Output (condensed)
Theory (W1–5) → frameworks, fairness, transparency, privacy, safety  
Cases (W6–10) → hiring bias, face recognition, genAI & copyright (law guest), AI & IHL  
Practice (W11–15) → group audit, debate (regulatory stances), final presentations  
Assessment: Midterm 30% / Group 40% / Final 30% (choice-based path allowed)
#### Layer Contribution
Turned design into an executable plan.

### Phase 4 — Reflective Intelligence
#### Prompt Example
“Evaluate inclusiveness, fairness, reflexivity, uncertainty disclosure.”
#### Sample Output
- Western-centrism risk → add Asia/Africa cases  
- Avoid implied “correct” stance in debates  
- Provide assessment choice (exam vs. extended report)  
- Maintain anonymous feedback & mid-course review
#### Layer Contribution
Shifted from “correct” to *ethically considerate* curriculum.

---

### 3) Inter-Layer Interactions
- **I→II**: Taxonomy informs audience-specific translations.  
- **II→III**: Context informs pacing, tasks, and assessment paths.  
- **IV→(II/III)**: Ethical audit injects global cases and choice-based grading.  
- **III→I**: Practical constraints refine scope of foundational topics.

### 4) Learning Points
- Contextualization and ethical audit are pivotal in education design.  
- Choice architectures (assessment options) reduce hidden bias and improve fairness.

### 5) Evaluation Metrics & Quality Rubric (Example-Specific)
| Layer | Effect Measure | Success Criteria (SLOs) | Quality Checks |
|---|---|---|---|
| I | Coverage index of core topics | ≥90% of baseline taxonomy present | Redundancy <10%, definitions consistent |
| II | Stakeholder relevance score (survey) | ≥4.2/5 for perceived relevance | Track-specific outcomes met |
| III | Execution fidelity | ≥85% sessions on plan; deliverables on time | Guest/session risk mitigations in place |
| IV | Ethical adequacy index | Bias findings addressed; uncertainty disclosed | Anonymous feedback loop active |

### 6) Cultural Adaptation Deep Dive
- **Alternative Cases**: Add Asian & African AI deployments; discuss community impact.  
- **Multilingual Support**: Bilingual glossaries; localized case briefs.  
- **Global Usability Test**: Think-aloud sessions (n≥6 across cultures), SUS ≥80, follow-up fixes logged.

---

## Example 2 | Medical Decision Support

### 1) Initial Diagnosis
| Question | Assessment | Missing Layer |
|---|---|---|
| Guideline synthesis? | ⭕ | — |
| Patient values understood? | ⚠️ Partial | II |
| Scenario execution plan? | ⚠️ Needs clarity | III |
| Ethical safeguards? | ⚠️ Needs audit | IV |

### 2) Layer-Specific Outputs
**I (Structural)**: Evidence table of options (survival, QoL, side effects).  
**II (Contextual)**: Values → “attend grandchild’s wedding (6 months).” Caregiver capacity, cost constraints.  
**III (Integrative)**: Three scenarios (A aggressive / B balanced / C palliative) with timeline & visit burden.  
**IV (Reflective)**: Ensure autonomy, disclose uncertainty (e.g., B ~70% chance to attend).

### 3) Inter-Layer Interactions
I→III (parameters), II→III (goals), IV→III (neutral framing & fairness).

### 4) Learning Points
- Medical “best” differs from “best-for-this-person.”  
- Present ranges/probabilities honestly.

### 5) Evaluation Metrics & Quality Rubric
| Layer | Measure | Success Criteria | Checks |
|---|---|---|---|
| I | Guideline concordance | 100% adherence or justified deviations | Evidence levels cited |
| II | Values capture score | Patient articulates priorities in own words | Family alignment documented |
| III | Scenario clarity | Patient can explain trade-offs unaided | Timeline & costs transparent |
| IV | Informed consent quality | Teach-back success; neutrality maintained | Uncertainty ranges shared |

### 6) Cultural Adaptation Deep Dive
- Alternatives: family-centered models; community consultation norms.  
- Multilingual consent forms; interpreter flow.  
- Usability: teach-back in two languages; comprehension ≥90%.

---

## Example 3 | Startup Hiring Strategy

### 1) Initial Diagnosis
| Question | Assessment | Missing Layer |
|---|---|---|
| Hiring framework defined? | ⚠️ Basic | I |
| Culture–candidate fit? | ❌ Not articulated | II |
| Process design? | ⚠️ Partial | III |
| Fairness audit? | ❌ Missing | IV |

### 2) Layer-Specific Outputs
**I**: Skills × Culture × Diversity matrix.  
**II**: Mission/values mapping; role narratives per team.  
**III**: Structured interviews, scorecards, calibrated panel; offer bands.  
**IV**: Blind résumé review; pass/fail rubrics; bias monitoring.

### 3) Inter-Layer Interactions
I→II (criteria to narratives), II→III (signals to instruments), IV→III (fairness gates).

### 4) Learning Points
- Structure reduces variance; calibration reduces bias.  
- Diversity goals must be embedded as process checkpoints.

### 5) Evaluation Metrics & Quality Rubric
| Layer | Measure | Success Criteria | Checks |
|---|---|---|---|
| I | Framework completeness | Competency map covers role demands | No orphan criteria |
| II | Signal validity | On-the-job success ↑ within 90 days | Retrospective correlation |
| III | Process reliability | Inter-rater κ ≥ 0.6; time-to-offer ≤ target | Debrief discipline |
| IV | Fairness index | Pass-rate parity within tolerance bands | Audit logs; appeal path |

### 6) Cultural Adaptation Deep Dive
- Alternate markets’ norms (e.g., seniority signals).  
- Multilingual scorecards; inclusive exercises.  
- Global candidate UX test; drop-off analysis.

---

## Cross-Domain Patterns
(unchanged in spirit; now informed by metrics and cultural checks)

---

## Try It Yourself
(unchanged; add space to define metrics & rubric per layer)

---

## Global Evaluation Rubric (All Examples)
Score each layer 1–5:

| Score | Descriptor | Indicators |
|---|---|---|
| 1 | Poor | Missing artifacts; unclear assumptions |
| 2 | Fair | Partial coverage; ad-hoc checks |
| 3 | Good | Baseline coverage; basic metrics tracked |
| 4 | Strong | Contextualized; measurable; feedback loops active |
| 5 | Excellent | Adaptively refined; culturally validated; audited for ethics |
