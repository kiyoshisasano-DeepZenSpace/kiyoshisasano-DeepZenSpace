---
title: "PLD Metrics Module â€” Quickstart Edition"
version: 2025.1
maintainer: "Kiyoshi Sasano"
status: stable
tags:
  - PLD
  - evaluation
  - drift metrics
  - applied UX
  - behavioral logging
---

# ðŸ“Š PLD Metrics Module â€” Quickstart Edition

This folder provides the **measurement layer** of the PLD Applied Interaction Framework.  
Its purpose is to help developers **log, evaluate, and visualize** how well an agent detects drift, performs repairs, reenters alignment, and maintains conversational stability.

This is the **minimal operational implementation** â€” fast to integrate and aligned with the full benchmark workflow used in the MultiWOZ (N=200) evaluation.

---

## 1. What This Module Enables

Once integrated, you can:

- Log system events using a **standard PLD JSON schema**
- Track Drift / Soft Repair / Hard Repair / Reentry / Outcome
- Generate stability dashboards and longitudinal comparisons
- Compare system versions and prompting strategies
- Integrate metrics into:

  - LangChain / LangGraph  
  - OpenAI Assistants API  
  - Rasa / Dialogue Managers  
  - Custom orchestration pipelines  

This module is designed for **runtime observability**, not offline annotation.

---

## 2. Module Structure

From the repository root:

```txt
quickstart/metrics/
â”‚
â”œâ”€â”€ README_metrics.md                 â† You are here
â”‚
â”œâ”€â”€ schemas/                          â† Canonical data definitions
â”‚   â”œâ”€â”€ pld_event.schema.json         â† PLD event-level schema
â”‚   â””â”€â”€ metrics_schema.yaml           â† Field dictionary + definitions
â”‚
â”œâ”€â”€ datasets/                         â† Example data
â”‚   â””â”€â”€ pld_events_demo.jsonl         â† Sample PLD log file
â”‚
â”œâ”€â”€ guides/                           â† Implementation guidance
â”‚   â””â”€â”€ drift_event_logging.md        â† How to instrument runtime logging
â”‚
â”œâ”€â”€ reports/                          â† Example evaluation outputs
â”‚   â””â”€â”€ pld_events_demo_report.md     â† Summary analysis of the demo dataset
â”‚
â””â”€â”€ dashboards/                       â† Visualization presets
    â””â”€â”€ reentry_success_dashboard.json â† Metrics dashboard template
```
## 3. Core Metric Categories

| Metric Class | Meaning | Examples |
|--------------|---------|----------|
| Drift Metrics | Frequency and type of divergence | D1_information_drift, D3_intent_drift |
| Repair Metrics | Whether the system corrects locally or resets | Soft Repair Rate / Hard Repair Rate |
| Reentry Metrics | Whether the agent stabilizes after repair | Reentry Success Rate (RE1â€“RE3) |
| Timing Metrics | Latency effects on perceived reliability | Avg Latency, High-Latency Threshold Events |
| Outcome Metrics | Completion trajectory | Complete / Partial / Abandoned / Reset |

These metrics align directly with:

- `docs/`
- `quickstart/operator_primitives/`
- `quickstart/patterns/`
- `metrics_studies/multiwoz_2.4_n200/`

---

## 4. Adoption Workflow

| Phase | Action | Reference |
|-------|--------|-----------|
| Step 1 | Instrument logging | `schemas/pld_event.schema.json` |
| Step 2 | Validate schema compliance | `schemas/metrics_schema.yaml` |
| Step 3 | Produce sample log | `datasets/pld_events_demo.jsonl` |
| Step 4 | Run evaluation | `reports/pld_events_demo_report.md` |
| Step 5 | Visualize stability | `dashboards/reentry_success_dashboard.json` |

This workflow ensures consistency across agent versions, prompting strategies, and orchestration architectures.

---

## 5. Quick Interpretation Rules

| Signal | Meaning | Suggested Action |
|--------|---------|------------------|
| Drift â†‘ + Soft Repair â†‘ | System is interpretable but imprecise | Improve constraints, grounding, or tool conditioning |
| Drift â†‘ + Hard Repair â†‘ | Architecture or memory mismatch | Review UX pacing, context access, or tool spec |
| Reentry Success â†“ | Repair is not stabilizing | Adjust confirmation pattern or reentry checkpoint |
| Outcome Complete â†‘ + Latency â†‘ | Stable but slow | Tune streaming, caching, or pacing strategy |

Use these rules as a runtime debugging baseline, especially during early prototyping.

---

## 6. Metrics â†’ Action Matrix (Runtime Decision Guide)

Once events are logged and visualized, use this matrix to determine next steps.  
It connects **observation â†’ system response â†’ improvement path.**

| Observed Pattern (Log) | Severity | Recommended Action | Notes |
|------------------------|----------|--------------------|-------|
| Low drift frequency + high repair success | Low | Continue normal execution | System is stable |
| Repeated soft repairs on same task | Medium | Improve prompts or constraints | Often signals weak grounding |
| Frequent hard repairs | High | Review memory, tools, or orchestration logic | Indicates structural issue |
| Reentry failure after repair | Critical | Trigger fallback strategy or session reset | Prevents infinite loops |
| High abandonment rate | Critical | Analyze UX timing + failure messaging | Often perception, not logic |

This matrix should become part of your AgentOps workflow and CI evaluation strategy.

---

## 7. When to Expand

Expand this module when:

- Evaluating > 200 real interactions  
- Comparing multiple model or runtime strategies  
- Testing production traffic  
- Tracking repair policies over time in continuous deployment  

If you are still validating system fit or integration strategy:

â†’ **This Quickstart module is sufficient.**

---

## 8. License

Creative Commons â€” **CC BY-NC 4.0**  
Â© 2025 â€” DeepZenSpace  
Maintainer: **Kiyoshi Sasano**

---

> **PLD Metrics is not generic analytics â€”  
it is behavioral instrumentation.  
It measures whether the agent remains aligned with the interaction contract.**
