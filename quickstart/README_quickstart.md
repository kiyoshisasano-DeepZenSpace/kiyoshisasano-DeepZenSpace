---
title: "PLD Applied Quickstart Kit"
version: "2025.2"
status: stable
maintainer: "Kiyoshi Sasano"
tags:
  - PLD
  - LLM Agents
  - Drift Control
  - Runtime Repair
  - Telemetry-Driven AI
---

# ğŸš€ PLD Applied Quickstart Kit  
**For LLM Agents, Orchestrators, and Conversational Systems (2025 Edition)**  

> PLD is best understood **through runtime experience â€” not theory alone.**

This kit provides everything needed to implement **Phase Loop Dynamics (PLD)** in a live AI agent:

- Drift detection and classification  
- Repair selection (soft â†’ hard)
- Reentry confirmation and stabilization  
- Failover rules and bounded execution  
- Metrics, dashboards, and runtime governance  

PLD is not a prompting trick.  
It is a **runtime interaction control model** for applied AI systems.

---

## ğŸ Start Here â€” Run the Minimal Runtime

`hello_pld_runtime.py` is the simplest runnable demonstration of the full PLD loop.

```bash
python hello_pld_runtime.py
```

Try custom input:

```bash
python hello_pld_runtime.py "Can we switch topics and talk about cooking?"
```

Run all example scenarios:

```bash
python hello_pld_runtime.py --examples
```

ğŸ’¡ This establishes intuition for the runtime lifecycle:
```bash
Drift â†’ Repair â†’ Reentry â†’ Continue
```

---

ğŸ”§ Next: Run the Real Engine

Once you understand the runtime feel, activate the full controller:

```bash
python run_minimal_engine.py
```

âœ” Uses the real PLD policies
âœ” Logs events using the canonical schema
âœ” Produces a trace of decisions and alignment events

This is the first verification checkpoint that your environment is correctly wired.

---

## 1 â€” Why PLD Exists

LLMs rarely fail because they lack knowledge â€”  
they fail because they lose **task alignment across turns.**

| Failure Mode | Result |
|--------------|--------|
| Loss of grounding | User distrust |
| Incorrect propagation of prior context | Cascading errors |
| Tool/API mismatch without acknowledgement | Workflow stalls |
| Full resets | Lost session state & user confidence |

PLD formalizes the lifecycle to prevent collapse:

> **The goal is not correctness â€” the goal is recoverable alignment**.

---

## 2 â€” How to Use This Folder

| Step  | Location               | What You Learn                                 |
| ----- | ---------------------- | ---------------------------------------------- |
| **1** | `overview/`            | High-level mental model                        |
| **2** | `hello_pld_runtime.py` | First hands-on runtime experience              |
| **3** | `operator_primitives/` | Drift â†’ Repair â†’ Reentry linguistic operators  |
| **4** | `patterns/`            | Best-practice runtime behavior and UX phrasing |
| **5** | `integration_recipes/` | LangGraph / Rasa / OpenAI Assistants wiring    |
| **6** | `metrics/`             | Telemetry, dashboards, and stability analysis  |

â¡ï¸ For framework bindings:
`quickstart/patterns/04_integration_recipes/`

---

## 3 â€” Core Runtime Lifecycle

PLD operates as a deterministic runtime loop:
```python
User Turn
   â†“
Drift Detected? â”€â”€ No â”€â”€â–¶ Continue
         â”‚
        Yes
         â†“
Select Repair â†’ Apply â†’ Reentry Check â†’ Continue / Escalate / Failover
```
Each step produces structured telemetry aligned with:
```pgsql
quickstart/metrics/schemas/pld_event.schema.json
```

---

## 4 â€” Example Logged Event

```json
{
  "session_id": "MWZ-001",
  "turn_id": 4,
  "event_type": "drift_detected",
  "pld": {
    "phase": "drift",
    "code": "D2_context",
    "confidence": 0.92
  },
  "runtime": {
    "latency_ms": 3120,
    "source": "assistant"
  }
}
```

Compatible with:
- LangGraph state stores
- OpenAI Assistants streaming telemetry
- Tool traces + RAG observability systems
- OpenTelemetry spans

---

## 5 â€” What Gets Measured

| Metric                    | Meaning                               |
| ------------------------- | ------------------------------------- |
| Drift Frequency           | Stability baseline                    |
| Soft vs Hard Repair Ratio | Efficiency vs escalation pressure     |
| Reentry Success Rate      | Ability to stabilize after correction |
| Failover Trigger Rate     | Safety boundary activation            |
| Latency-Induced Drift     | UX-performance dependency             |
| Outcome Distribution      | Completion vs abandonment             |

These power:

- model comparisons
- policy tuning
- architecture iteration
- UX alignment studies
- Behavior is only real when measurable.

---

## 6 â€” Evaluation Dataset (Optional But Useful)

The dataset at:
```bash
analytics/multiwoz_2.4_n200/
```

allows you to:
- benchmark models
- test pattern changes
- measure policy improvements

This supports the cycle:
```perl
prototype â†’ evaluate â†’ tune â†’ redeploy
```

---

## ğŸ” The PLD Feedback Loop

Once everything is wired:
```pgsql
Runtime â†’ Logging â†’ Metrics â†’ Dashboard â†’ Adjust Policy â†’ Update Patterns â†’ Rerun
```
This enables **governable agent behavior**.

---

### License & Attribution

Creative Commons **BY-NC 4.0**  
Maintainer: **Kiyoshi Sasano**

---

> PLD is not static rules â€”  
> it is a sustained discipline for maintaining aligned shared reality with the user.

